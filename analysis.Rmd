---
title: "Feature selection analysis"
subtitle: "Reproducibility package for the paper ..."
author: "R. Torkar and R. Berntsson Svensson"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
margin_references: true
bibliography: refs.bib
link-citations: yes
csl: ieee-transactions-on-software-engineering.csl
header-includes:
  - \usepackage{amsmath}
  - \DeclareMathOperator{\logit}{logit}
---

```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)

library(openxlsx)
library(ggplot2)
library(ggthemes)
library(brms)
library(bayesplot)
library(dplyr)
library(kableExtra)
library(patchwork)

options(mc.cores = parallel::detectCores()) # set num cores

theme_set(theme_tufte())
theme_update(
  panel.background = element_rect(fill = "#fffff8", colour = "#fffff8"),
  plot.background = element_rect(fill = "#fffff8", colour = "#fffff8")
  )
```

\newcommand{\logit}{\operatorname{logit}}

# Introduction
First prepare the data, check for `NAs` and look at some descriptive statistics. Since we're using Excel we should be very careful when loading the data to see that nothing goes wrong (data manipulated in an arbitrary way, uncompatible data types, etc.)
```{r}
d <- read.xlsx("Features.xlsx", sheet = "Features")
```

This is how the data looks like. In short, 11110 rows and 10 variables.

```{r}
str(d)
```

Each row has a unique `ID`. `State`, which is our outcome (dependent variable), shows how far the feature got in the process. It is of an ordered categorical type, which should be modeled with a cumulative or adjacent category likelihood. There are seven categories:

1. `Elicited, Dropped`
2. `Elicited, Prio, Dropped`
3. `Elicited, Prio, Planned, Dropped`
4. `Elicited, Prio, Planned, Implemented, Dropped`
5. `Elicited, Prio, Planned, Implemented, Tested, Dropped`
6. `Elicited, Prio, Planned, Implemented, Tested, Released`

`Team.priority` is the relative priority the feature got, $\mathbb{N} = \{0,\ldots,1000\}$. `Critical.feature` is a simple 'Yes'/'No' answer ($\mathbb{Z}_2$). 
`Business.value` and `Customer.value` are also ordered categorical with three levels and a fourth level called 'No value': 

1. `No value`
2. `Valuable`
3. `Important`
4. `Critical`

`Stakeholders` have integers, i.e., $\mathbb{N} = \{0,\ldots,10\}$, and `Key.customers` the same, but with a different set, i.e., $\mathbb{N} = \{0,\ldots,60\}$. 

Finally, `Dependency` is $\mathbb{Z}_2$, while `Architects.involvement` is ordered categorical: 

1. `None`
2. `Simple`
3. `Monitoring`
4. `Active Participation`
5. `Joint Design`

All ordered categorical predictors (independent variables) should be modeled as monotonic or category-specific effects [@burkner20monotonic].

```{r}
table(is.na(d))
```

No `NAs` in the dataset. However, that doesn't mean that we don't have `NAs`. Some of the coding can be a representation of `NA`, e.g., 'No value'. In this particular case we know that 'No value' and 'None' in the dataset actually are values and not a representation of `NA`.

Finally, we should set correct data types on all predictors.
```{r}
# ordered categorical
d$State <- factor(d$State, 
                  levels = c("Elicited, Dropped", 
                             "Elicited, Prio, Dropped", 
                             "Elicited, Prio, Planned, Dropped", 
                             "Elicited, Prio, Planned, Implemented, Dropped", 
                             "Elicited, Prio, Planned, Implemented, Tested, Dropped", 
                             "Elicited, Prio, Planned, Implemented, Tested, Released"), 
                  ordered = TRUE)

d$Business.value <- factor(d$Business.value, 
                           levels = c("No value",
                                      "Valuable",
                                      "Important",
                                      "Critical"), 
                           ordered = TRUE)

d$Customer.value <- factor(d$Customer.value, 
                           levels = c("No value",
                                      "Valuable",
                                      "Important",
                                      "Critical"), 
                           ordered = TRUE)

d$Architects.involvement <- factor(d$Architects.involvement,
                                   levels = c("None",
                                              "Simple",
                                              "Monitoring",
                                              "Active Participation",
                                              "Joint Design"), 
                                   ordered = TRUE)

# binary
d$Critical.feature <- ifelse(d$Critical.feature == 'Yes', 1, 0)
d$Dependency <- ifelse(d$Dependency == 'Yes', 1, 0)

```

## Descriptive statistics

```{r desc_stat, fig.margin=TRUE, echo=FALSE, warning=FALSE, message=FALSE}

ggplot(d, aes(x=State)) + 
  geom_histogram(stat = "count") +
  xlab("") + 
  ylab("Num. features") +
  ggtitle("Predictor 'State'") +
  theme(axis.text.x = element_text(angle=15, hjust=1),
        plot.title = element_text(hjust = 0.5, face = "bold"))

ggplot(d, aes(x=Team.priority)) +
  geom_histogram() +
  xlab("") + ylab("") +
  ggtitle("Predictor 'Team.priority'") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

par(bg="#fffff8")
barplot(table(d$Critical.feature), main = "Variable 'Critical.feature'")

ggplot(d, aes(x=Business.value)) + 
  geom_histogram(stat = "count") +
  xlab("") + 
  ylab("Num. features") +
  ggtitle("Predictor 'Business.value'") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

ggplot(d, aes(x=Customer.value)) + 
  geom_histogram(stat = "count") +
  xlab("") + 
  ylab("Num. features") +
  ggtitle("Predictor 'Customer.value'") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

par(bg="#fffff8")
barplot(table(d$Stakeholders), main = "Variable 'Stakeholders'")

par(bg="#fffff8")
barplot(table(d$Key.customers), main = "Variable 'Key.customers'")

par(bg="#fffff8")
barplot(table(d$Dependency), main = "Variable 'Dependency'")

ggplot(d, aes(x=Architects.involvement)) + 
  geom_histogram(stat = "count") +
  xlab("") + 
  ylab("Num. features") +
  ggtitle("Predictor 'Architects.involvement'") +
  theme(axis.text.x = element_text(angle=15, hjust=1),
        plot.title = element_text(hjust = 0.5, face = "bold"))

```

We now have a data frame, `d`, which has all variable types correctly set.
```{r}
str(d)
```

Let's plot our outcome and predictors so we get a feeling for the distributions. In the margin you will find all plotted.

First, we see that for `State` approximately as many features are released (final stage) as dropped in the first state. We also see that it drops off after the initial state. 

For `Team.priority` many features have zero in priority ($5139$), and then there's a bunch of them ($1516$) that have priority set to the maximum value, i.e., $1000$.

For `Critical.feature` we have an emphasis on 'No'. 

Concerning `Business.value` and `Customer.value` they are fairly similar in their respective distribution (as one would expect). 

For `Stakeholder` and `Key.customers` we see an emphasis on lower numbers, while for `Dependency` an emphasis on 'No'. 

Finally, for `Architects.involvement` we see that in the absolute majority of the cases architects are not involved.

In short, it looks sort of what one would expect, i.e., it's not hard to find answers to why the plots look the way they do. 

However, before we continue, we should standardize some of our predictors so the sampling will be easier, i.e., we simply do $(x - \bar{x})/\sigma_x$, then simply multiplying with $\sigma_x$ and adding the mean, will allow us to get back to our untransformed scale. It's good practice to store this in new variables and suffix them with `_s`. At the same time, let's give our variables abbreviated names.

```{r}
# standardize
d$prio_s <- scale(d$Team.priority)
d$sh_s <- scale(d$Stakeholders)
d$kc_s <- scale(d$Key.customers)

# abbreviate names
d$crit <- d$Critical.feature
d$b_val <- d$Business.value
d$c_val <- d$Customer.value
d$dep <- d$Dependency
d$arch <- d$Architects.involvement
```

## Initial model comparison

First we sample a model that only has a population-level intecept (our null model, $\mathcal{M}_0$). Then we sample models using a cumulative likelihood (first w/o modeling predictors as monotonic) [@burkner19ordinal]. Once we've sampled our models we then use LOO to compare each models' relative out of sample prediction capabilities [vehtari17loo].

```{r null_model, cache=TRUE, warning=FALSE, message=FALSE}
M0 <- brm(State ~ 1, family = cumulative, data = d, refresh = 0)

M1 <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = cumulative, data = d, refresh = 0)

# Since it's cumulative likelihood we need to model as monotonic.
M2 <- brm(State ~ 1 + prio_s + crit + mo(b_val) + mo(c_val) + 
                 sh_s + kc_s + dep + mo(arch),
          family = cumulative, data = d, refresh = 0)
```

Add `LOO` fit criterion to the models and then compare their respective out of sample predictions.

```{r loo_comp, cache=TRUE, warning=FALSE, message=FALSE}
M0 <- add_criterion(M0, criterion = "loo")
M1 <- add_criterion(M1, criterion = "loo")
M2 <- add_criterion(M2, criterion = "loo")
(l <- loo_compare(M0, M1, M2))
```

`LOO` puts $\mathcal{M}_2$ as no. 1. If we assume a $z_{\text{95%}}$-score of $1.96$ it's clear that zero is in the interval and that $\mathcal{M}_2$ does not have much of an advantage, i.e., $\text{CI}_{z_{95\%}}$ [`r round(l[2,1] + c(-1,1) * l[2,2] * 1.96, 2)`]. We can conclude this matter by saying that adding predictors to $\mathcal{M}_0$ *clearly* has a significant effect, but adding monotonic effects to our model does not. Interpreting monotonic effects is harder so in this case, since it has no significant effect, we will not include it in the model.

If we would be interested in refining our model for out of sample prediction purposes we could conduct variable selection. However, in this particular case we are interested in each predictor's effect, so we'll keep them all, and decide to use $\mathcal{M}_1$ as our target model, $\mathcal{M}$, for now.

## Prior predictive checks

Before we start to use our data to do inferences we should think about our priors. Our outcome of interest is `State`, which is ordered categorical data and as such we will model it using a cumulative likelihood (as seen in the previous section).

Let's see what priors we would get with a full model where we include all other variables as predictors. We'll model `b_val`, `c_val`, and `arch` as population-level effects.

We have 15 $\beta$ and 5 $\alpha$ parameters that we need to set a prior on. Since we use a $\logit$ link for our model (to translate to the probabilistic scale) it is hard to conceptually grasp the effect of our priors, i.e., we need to visualize them.

First, we tried weakly regularizing priors on $\beta$, while we set the same on our five $\alpha$ parameters.^[Basically $\mathcal{N}(0,1)$] Then, after analyzing and comparing different priors we conclude that the following priors are suitable for this model.^[Providing all steps in a prior sensitivity analysis is beyond the scope of this document and seldom reported in scientific literature.]

```{r priors, warning=FALSE, message=FALSE}
p <- get_prior(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = cumulative, data = d)

# Beta (we have fifteen of them)
# M(0,0.1) might seem very strict, but (15*0.1)^2 = 2.25
p$prior[1] <- "normal(0, 0.1)"

# Our priors on alpha (five of them; one for each border between 
# Likert scale 1,...,6)
p$prior[17] <- "normal(0, 2)"
```

Let's sample from the priors only (it's enough to use one chain here), and then check against the data to see that they are wide enough.

```{r m_prior, cache=TRUE, warning=FALSE, message=FALSE}
M_prior <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = cumulative, data = d, prior = p, sample_prior = "only", 
          chains = 1, refresh = 0)
```

```{r ppc_plot, cache=TRUE, fig.margin=TRUE}
pp_check(M_prior, type = "bars", nsamples = NULL)
```

If we sample from our prior predictive distribution, $y_{\text{rep}}$, and plot it with our empirical data, $y$, we'll see how the priors cover our data. As is evident (see right), by looking at the intervals, our priors seems to be relaxed and should not influence the outcome.

# Inference

Let's now sample from our model using our priors from above.

```{r model_m, cache=TRUE, warning=FALSE, message=FALSE}
M <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch, 
         family = cumulative, data = d, prior = p, chains = 4, refresh = 0)
```

## Posterior predictive check

```{r ppc, echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE, fig.margin=TRUE}
pp_check(M, type = "bars", nsample = 100) +
  scale_x_continuous(breaks=c(seq(1:6)), labels=as.character(levels(d$State))) +
  theme(axis.text.x = element_text(angle=15, hjust=1))
```

Again, plotting our model's predictions, $y_{\text{rep}}$, vs. our empirical values, $y$, we see that the model does a good job in estimating the categories. The credible intervals are tight (a lot of data). If we look at our previous plot we can clearly see, when we compare with the plot to the right, that our data has completely swamped the priors. We see some under- and overestimation in some categories but these are minor.

## Diagnostics

Our caterpillar plots look good for all our 21 parameters that the model estimated.

```{r caterpillar, echo=FALSE, fig.width=12, fig.height=8, fig.fullwidth=TRUE}
mcmc_trace(M) + legend_none()
```

Our $\widehat{R}$ and effective sample size (neff) look good.

```{r}
# should be < 1.01
max(rhat(M))

# should be > 0.1
min(neff_ratio(M))
```

There is no perfect model, but we could claim that this is a useful model. We would even consider taking it out for a dinner.^["Essentially, all models are wrong, but some models are useful." &mdash;George E. P. Box]

## Estimates of parameters

If we now focus on our estimates we see that our 'Intercepts', which in this case are the borders between two categories in our outcome, are precisely estimated. However, to make sense of them we need to transform the estimates, since the values are on the $\logit$ scale. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
kable(round(fixef(M)[c(1:8,11,14:17),], digits=2), caption="Our population-level estimates (fixed effects) from our model $\\mathcal{M}$. Note that the values are on $\\logit$ scale.") %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

Let's take the first parameter, `Intercept[1]`, which was estimated to `r round(fixef(M)[1,1],2)`, i.e., 

```{r} 
inv_logit_scaled(fixef(M)[1,1])
```

What does `r round(inv_logit_scaled(fixef(M)[1,1]), 2)` mean? Well, we have a ordered categorical outcome. So `r round(inv_logit_scaled(fixef(M)[1,1]), 2) * 100`% of the probability mass, with a 95% credible interval of [`r round(inv_logit_scaled(fixef(M)[1,3]), 2)`, `r round(inv_logit_scaled(fixef(M)[1,4]), 2)`], was assigned to the first category: `Elicited, Dropped`. For `Intercept[2]`, `r round(inv_logit_scaled(fixef(M)[2,1]), 2)` [`r round(inv_logit_scaled(fixef(M)[2,3]), 2)`, `r round(inv_logit_scaled(fixef(M)[2,4]), 2)`] was assigned to: `Elicited, Dropped` **and** `Elicited, Prio, Dropped`.

Let's turn our attention to the other parameters. The `prio_s` parameter, `r round(fixef(M)[6,1], 2)`, would then become `r round(inv_logit_scaled(fixef(M)[6,1]), 2)` when transformed. But remember the suffix `_s`! We need to multiply with $\sigma_x$  and add $\bar{x}$ from the data, which leads to an estimate of `r round(inv_logit_scaled(fixef(M)[6,1]) * attr(d$prio_s, "scaled:scale") + attr(d$prio_s, "scaled:center"), 2)` [`r round(inv_logit_scaled(fixef(M)[6,3]) * attr(d$prio_s, "scaled:scale") + attr(d$prio_s, "scaled:center"), 2)`, `r round(inv_logit_scaled(fixef(M)[6,4]) * attr(d$prio_s, "scaled:scale") + attr(d$prio_s, "scaled:center"), 2)`]. 

However, looking at point estimates is, quite frankly, not terribly useful. Let's plot the posterior probability densitities for our population-level estimates on the $\logit$ scale, disregarding `Intercept`$[1,\ldots,5]$.

```{r mcmc_areas, echo=FALSE}
mcmc_areas_ridges(M, 
                  pars = c("b_prio_s","b_crit","b_b_val.L","b_c_val.L","b_sh_s","b_kc_s","b_dep","b_arch.L"),
                  prob_outer = 1, prob = 0.95) +
  vline_0(size=0.3)
```

Examining the above plot, from top to bottom, we can say that on the 95%-level, the first three parameters are clearly positive and do not cross $0$. The fourth parameter, `Customer value`, is actually significant, with the following 95% credible intervals [`r round(fixef(M)[11,3], 2)`, `r round(fixef(M)[11,4], 2)`]. The fifth parameter is clearly negative. The sixth parameter, `Key customers`, is not significant. Finally, `Dependency` is positive, while `Architects involvement` is not significant.

To conclude what we've noticed so far: `Team priority`, `Critical feature`, `Business value`, `Stakeholders`, and `Dependency` are clearly significant on $\text{CI}_{95\%}$, while `Customer value` is borderline significant. `Architecture involvement` and `Key customers` are not significant.

## Conditional effects

Below we plot all conditional effects for our model. The colors represent the different categories, $1,\ldots,6$, for our outcome `State`. We are particularly interested in category 6 (pink), i.e., the final category which indicates a released feature.

1. `Elicited, Dropped`
2. `Elicited, Prio, Dropped`
3. `Elicited, Prio, Planned, Dropped`
4. `Elicited, Prio, Planned, Implemented, Dropped`
5. `Elicited, Prio, Planned, Implemented, Tested, Dropped`
6. `Elicited, Prio, Planned, Implemented, Tested, Released`

```{r cond_effects, echo=FALSE}
conditional_effects(M, categorical = TRUE)
```

## Take-aways concerning released features
One important question we would like to have an answer to is which independent variable(s) contributes more for a feature to, ultimately, be *released*, i.e., is it priority, criticality, business or customer value, number of stakeholders, number of key customers, having dependencies, and/or the level of architect involvement? In the above plots the answer to our question can be found, without even having to conduct any statistical tests and examining $p$-values.

Concerning `Priority` we see that it has a very large effect for State 6 (i.e., a feature being released). The higher the priority the more probability mass is set on State 6. In the end it has close to 70% of the probability mass, while the other states are not even close.

Concerning `Criticality` we see very much the same effect, albeit the uncertainty increases also. States 3 and 6 have, together, more than 50% of the probability mass.

`Business value` and `Customer value` play a small role if we look at the plots above. However, one thing we clearly see in the `Stakeholders` plot is that State 6 has very little probability mass when number of stakeholders increase.

`Key customers` doesn't tell us much due to the very large uncertainty in State 6. For `Dependencies` not much changes when it increases.

Finally, for `Architects involvement` we see that the probability for State 6 increases when going from `none` to `Simple`, but then it remains virtually the same for the other categories.

In short, `Priority` and `Criticality` have the largest effects, while the rest don't matter all too much.



