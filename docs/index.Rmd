---
title: "Feature selection analysis of 11110 requirements"
subtitle: "Replication package"
author: "R. Torkar and R. Berntsson Svensson"
date: "First revision 2020-01-17. This revision `r Sys.Date()`."
output:
  bookdown::html_document2:
    toc: true
    toc_float: true
    toc_depth: 1
    includes:
      before_body: footer.html
bibliography: refs.bib
link-citations: true
csl: elsevier-harvard.csl
header-includes:
  - \usepackage{amsmath}
  - \DeclareMathOperator{\logit}{logit}
---

<!-- PLEASE READ THE CODE CHUNKS BELOW-->
```{r setup, include=FALSE}
start.time <- Sys.time()
options(htmltools.dir.version = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_chunk$set(cache=TRUE)

# set this to num cores on your computer!
CORES = 4 

library(openxlsx)
library(ggplot2)
library(ggthemes)
library(bayesplot)
library(tidyverse)
library(kableExtra)

# !!! Make sure to install cmdstanr and then cmdstan !!!
# > remotes::install_github("stan-dev/cmdstanr")
# > library(cmdstanr)
# > install_cmdstan(cores = 4) 
# then,
options(brms.backend="cmdstanr")
options(mc.cores = CORES) # set num cores
library(brms)
```

# Introduction

## Synthetic data

In this replication package we have used the empirical data, which is under NDA. However, one can use synthetic data to evaluate our approach and reach the same conclusions.

Here we show how to load the synthetic dataset (from the `data/` directory in the GitHub [repository](https://github.com/torkar/feature-selection-RBS)), which can then be used in the analysis.

```{r, eval=FALSE}
d <- readRDS("data/data.rds")
```

The above line is currently not executed, since we will continue to run this analysis with the empirical data. However, by executing the above line, and *not* executing the next line, the analysis can be executed as-is with synthetic data.

## Data cleaning

First prepare the data, check for `NAs` and look at some descriptive statistics. Since we're using Excel we should be very careful when loading the data to see that nothing goes wrong (data manipulated in an arbitrary way, incompatible data types, etc.)

```{r}
# Make sure to execute the previous statement, and not this one, if you want to 
# re-run the analysis
d <- read.xlsx("data/Features.xlsx", sheet = "Features")
```

This is how the empirical data looks like. In short, `r nrow(d)` rows and `r ncol(d)` variables.

```{r}
str(d)
```

Each row (requirement) has a unique `ID`. The `State`, which is our outcome (dependent variable), shows how far the feature survived in the requirements process. It is of an ordered categorical type, which should be modeled with some type of <font style="font-family: serif">Cumulative</font> likelihood. There are seven categories:

1. `Elicited, Dropped`
2. `Elicited, Prio, Dropped`
3. `Elicited, Prio, Planned, Dropped`
4. `Elicited, Prio, Planned, Implemented, Dropped`
5. `Elicited, Prio, Planned, Implemented, Tested, Dropped`
6. `Elicited, Prio, Planned, Implemented, Tested, Released`

`Team.priority` is the relative priority the feature got, $\mathbb{N} = \{0,\ldots,1000\}$. `Critical.feature` is a simple 'Yes'/'No' answer ($\mathbb{Z}_2$). `Business.value` and `Customer.value` are also ordered categorical with three levels and a fourth level called 'No value', which does not imply missingness: 

1. `No value`
2. `Valuable`
3. `Important`
4. `Critical`

`Stakeholders` have integers, i.e., $\mathbb{N} = \{0,\ldots,10\}$, and `Key.customers` the same, but with a different set, i.e., $\mathbb{N} = \{0,\ldots,60\}$. 

Finally, `Dependency` is $\mathbb{Z}_2$, while `Architects.involvement` is ordered categorical: 

1. `None`
2. `Simple`
3. `Monitoring`
4. `Active Participation`
5. `Joint Design`

All ordered categorical predictors (independent variables) can be modeled as monotonic or category-specific effects, if necessary [@burkner20monotonic].

```{r}
table(is.na(d))
```

No `NAs` in the dataset. However, that doesn't necessarily mean that we don't have `NAs`. Some of the coding can be a representation of `NA`, e.g., 'No value'. In this particular case we know that 'No value' and 'None' in the dataset actually are values and not a representation of `NA`.

Finally, we set correct data types.

```{r}
# ordered categorical
d$State <- factor(d$State, 
                  levels = c("Elicited, Dropped", 
                             "Elicited, Prio, Dropped", 
                             "Elicited, Prio, Planned, Dropped", 
                             "Elicited, Prio, Planned, Implemented, Dropped", 
                             "Elicited, Prio, Planned, Implemented, Tested, Dropped", 
                             "Elicited, Prio, Planned, Implemented, Tested, Released"), 
                  ordered = TRUE)

# make sure we have integers for the other ordered categorical
d$Business.value <- factor(d$Business.value, 
                           levels = c("No value",
                                      "Valuable",
                                      "Important",
                                      "Critical"), 
                           ordered = TRUE)


d$Customer.value <- factor(d$Customer.value, 
                           levels = c("No value",
                                      "Valuable",
                                      "Important",
                                      "Critical"), 
                           ordered = TRUE)

d$Architects.involvement <- factor(d$Architects.involvement,
                                   levels = c("None",
                                              "Simple",
                                              "Monitoring",
                                              "Active Participation",
                                              "Joint Design"), 
                                   ordered = TRUE)
```


## Descriptive statistics

First, we see that for `State` approximately as many features are released (final stage) as dropped in the first state. We also see that it drops off after the initial state. 

```{r echo=FALSE}
ggplot(d, aes(x=as.factor(State))) + 
  geom_bar() +
  xlab("") + 
  ylab("Num. features") +
  ggtitle("Predictor: State") +
  theme(axis.text.x = element_text(angle=15, hjust=1))
```

For `Team.priority` many features have zero in priority ($5139$), and then there's a bunch of them ($1516$) that have priority set to the maximum value, i.e., $1000$.

```{r echo=FALSE}
ggplot(d, aes(x=Team.priority)) +
  geom_histogram(bins = 30) +
  xlab("") + 
  ylab("") +
  ggtitle("Predictor: Team priority")
```

For `Critical.feature` we have a clear emphasis on 'No'. 

```{r echo=FALSE}
ggplot(d, aes(x=Critical.feature)) +
  geom_bar() +
  xlab("") +
  ylab("") +
  ggtitle("Predictor: Critical feature")
```

Concerning `Business.value` and `Customer.value` they are fairly similar in their respective distribution (as one would expect). 

```{r echo=FALSE}
ggplot(d, aes(x=Business.value)) + 
  geom_bar() +
  xlab("") + 
  ylab("Num. features") +
  ggtitle("Predictor: Business value")

ggplot(d, aes(x=Customer.value)) + 
  geom_bar() +
  xlab("") + 
  ylab("Num. features") +
  ggtitle("Predictor: Customer value")
```

For `Stakeholder` and `Key.customers` we see a strong emphasis on lower numbers, while for `Dependency` a clear emphasis on 'No'. 

```{r echo=FALSE}
ggplot(d, aes(x=as.factor(Stakeholders))) +
  geom_bar() +
  xlab("Num. stakeholders") +
  ylab("Frequency") +
  ggtitle("Predictor: Stakeholders")

ggplot(d, aes(x=as.factor(Key.customers))) +
  geom_bar() +
  xlab("Num. key customers") +
  ylab("") +
  ggtitle("Predictor: Key customers")
  
ggplot(d, aes(x=Dependency)) +
  geom_bar() +
  xlab("") +
  ylab("") +
  ggtitle("Predictor: Dependency")
```

Finally, for `Architects.involvement` one can see that in the absolute majority of the cases architects are not involved.

```{r echo=FALSE}
ggplot(d, aes(x=Architects.involvement)) + 
  geom_bar() +
  xlab("") + 
  ylab("Num. features") +
  ggtitle("Predictor: Architects' involvement") +
  theme(axis.text.x = element_text(angle=15, hjust=1))
```

In short, it looks sort of what one would expect, i.e., it's not hard to find answers to why the plots look the way they do. 

However, before we continue, we should standardize some of our predictors so the sampling will be easier, i.e., we simply do $(x - \bar{x})/\sigma_x$, then simply multiplying with $\sigma_x$ and adding the mean, will allow us to get back to the original scale. 

```{r scale}
# standardize and abbreviated names and change types if need be
d$prio_s <- scale(d$Team.priority)
d$sh_s <- scale(d$Stakeholders)
d$kc_s <- scale(d$Key.customers)

d$b_val <- as.integer(d$Business.value) # use int as input
d$c_val <- as.integer(d$Customer.value)
d$arch <- as.integer(d$Architects.involvement)

# Dichotomous predictors. We can set these to 1/0, but generally speaking
# one should know what one is doing and be careful doing this!
d$Critical.feature <- ifelse(d$Critical.feature == 'Yes', 1, 0)
d$Dependency <- ifelse(d$Dependency == 'Yes', 1, 0)

# only abbreviate names
d$crit <- d$Critical.feature
d$dep <- d$Dependency
```

# Model design {.tabset .tabset-fade .tabset-pills}

## $\mathcal{M}_0$

Since our outcome is of an ordered categorical nature we have many options at our hands, some of which barely existed in Bayesian modeling a few decades ago. Our outcome reminds us of a survival model, i.e., a feature needs to survive in order to reach the next stage. Taking this into account we assume that the following type of models could be an option [@burkner19ordinal]:

* <font style="font-family: serif">Cumulative</font> (single underlying continuous variable) [@samejima97]. A model with no predictors ($\mathcal{M}_0$), with predictors ($\mathcal{M}_1$), and with monotonic predictors ($\mathcal{M}_2$).
* <font style="font-family: serif">Adjacent-category</font> (mathematically convenient) [@agresti10]. A model with predictors ($\mathcal{M}_3$).
* <font style="font-family: serif">Sequential</font> model (higher response category is possible only after all lower categories are achieved) [@tutz90]. A model with predictors ($\mathcal{M}_4$) and with category-specific predictors ($\mathcal{M}_5$).

The reason we want to use monotonic or category-specific modeling of predictors is that predictor categories will not be assumed to be equidistant with respect to their effect on the outcome [@burkner20monotonic].

The <font style="font-family: serif">Cumulative</font> family is very common so let us assume a null model, $\mathcal{M}_0$, which uses this likelihood, with no predictors. Later we will compare all our models to $\mathcal{M}_0$, to ensure that adding predictors improve out of sample predictions, i.e., if we can't improve when adding predictors we might as well do other things with our time.

### Prior predictive checks
Let us see what priors such a null model needs.

```{r M0-priprep}
(p <- get_prior(State ~ 1, 
               family = cumulative, 
               data = d))
```

```{r}
# Set a wide Normal(0,2) on the the intercepts (cutpoints for our 
# scale, which is on 6 levels, i.e., 5 cutpoints)
p$prior[1] <- "normal(0,2)"
```

Sample only from the priors.

```{r M0-pri}
# simplest cumulative model we can think of
M0 <- brm(State ~ 1, 
          family = cumulative, 
          data = d, 
          # threads = threading(4), # use if 16 CPUs
          prior = p,
          control = list(adapt_delta=0.9),
          sample_prior = "only",
          refresh = 0) # avoid printing sampling progress
```

Plot the priors $y_{\mathrm{rep}}$ vs. the empirical data $y$.

```{r}
pp_check(M0, type = "bars", nsamples = 250)
```

Evidently the medians are quite evenly set along the $x$-axis, and the uncertainty is fairly uniformly distributed among the categories $1,\ldots,6$ (the bars). In short, this is what we like to see.

### Sample with data

```{r M0}
M0 <- brm(State ~ 1, 
          family = cumulative, 
          data = d, 
          # threads = threading(4),
          prior = p,
          control = list(adapt_delta=0.9),
          refresh = 0)
```

### Diagnostics

Our caterpillar plots look good for all parameters the model estimated (i.e., they look like fat caterpillars when they four chains have mixed well).

```{r M0-caterpillar, echo=FALSE}
mcmc_trace(M0, regex_pars = "^b_") + legend_none()
```

Diagnostics such as divergences, tree depth and energy looks good. Additionally, our $\widehat{R}$ and effective sample size (ESS) look good.

```{r M0-diagnostics, echo=TRUE}
# Check divergences, tree depth, energy
rstan::check_hmc_diagnostics(eval(M0)$fit)

# Check rhat and ESS
if(max(rhat(eval(M0)), na.rm=T) >= 1.01) {
  print("Warning: Rhat >=1.01")
} else {
  print("All Rhat <1.01")
}

if(min(neff_ratio(eval(M0)), na.rm=T) <= 0.2) {
  print("Warning: ESS <=0.2")
} else {
  print("All ESS >0.2")
}
```

### Posterior predictive check

Let's look at a posterior predictive plot to see how well the model has estimated our $6$ levels we have in our outcome.

```{r}
pp_check(M0, type = "bars", nsamples = 250)
```

In short, very little uncertainty (i.e., the medians are quite well estimated). Let's leave $\mathcal{M}_0$ for now and add predictors to the next model, which you can read about by going back up and clicking on the tab $\[2.2 \mathcal{M}_1\]$.

## $\mathcal{M}_1$

Here we'll design a <font style="font-family: serif">Cumulative</font> model with predictors.

For this and the coming models we won't report on all the steps since it will take up too much space. However, rest assured, we have conducted all the steps, just as we did for $\mathcal{M}_0$.

### Prior predictive checks

Let's set sane priors that are uniform on the outcome space.

```{r M1-priprep}
p <- get_prior(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
               family = cumulative, 
               data = d)

# Set N(0,1) on \betas and N(0,2) on the cutpoints
p$prior[1] <- "normal(0,1)"
p$prior[10] <- "normal(0,2)"
```

Sample only from the priors.

```{r M1-pri}
M1 <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = cumulative, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          sample_prior = "only",
          refresh = 0)
```

Plot the priors $y_{\mathrm{rep}}$ vs. the empirical data $y$.

```{r}
pp_check(M1, type = "bars", nsamples = 250)
```

### Sample with data

```{r M1}
M1 <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = cumulative, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          refresh = 0)
```

### Diagnostics

```{r M1-diagnostics, echo=TRUE}
# Check divergences, tree depth, energy
rstan::check_hmc_diagnostics(eval(M0)$fit)

# Check rhat and ESS
if(max(rhat(eval(M0)), na.rm=T) >= 1.01) {
  print("Warning: Rhat >=1.01")
} else {
  print("All Rhat <1.01")
}

if(min(neff_ratio(eval(M0)), na.rm=T) <= 0.2) {
  print("Warning: ESS <=0.2")
} else {
  print("All ESS >0.2")
}
```

### Posterior predictive check

```{r}
pp_check(M1, type = "bars", nsamples = 250)
```

Slight overestimation in the third category and slight underestimations in the first two categories. Shouldn't be a problem but worth noting. The next model is $\mathcal{M}_2$.

## $\mathcal{M}_2$

A <font style="font-family: serif">Cumulative</font> model with monotonic predictors.

### Prior predictive checks

```{r M2-priprep}
p <- get_prior(State ~ 1 + prio_s + crit + mo(b_val) + mo(c_val) + sh_s + 
                 kc_s + dep + mo(arch),
               family = cumulative, 
               data = d)

p$prior[1] <- "normal(0,1)"
p$prior[10] <- "normal(0,2)"
p$prior[16:18] <- "dirichlet(2)" # prior for ordered categorical
```

```{r M2-pri}
M2 <- brm(State ~ 1 + prio_s + crit + mo(b_val) + mo(c_val) + sh_s + kc_s + 
            dep + mo(arch),
          family = cumulative, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          sample_prior = "only",
          refresh = 0)
```

```{r}
pp_check(M2, type = "bars", nsamples = 250)
```

### Sample with data

```{r M2}
# simplest cumulative model we can think of
M2 <- brm(State ~ 1 + prio_s + crit + mo(b_val) + mo(c_val) + sh_s + kc_s + 
            dep + mo(arch),
          family = cumulative, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          refresh = 0)
```

### Diagnostics

```{r M2-diagnostics, echo=TRUE}
# Check divergences, tree depth, energy
rstan::check_hmc_diagnostics(eval(M0)$fit)

# Check rhat and ESS
if(max(rhat(eval(M0)), na.rm=T) >= 1.01) {
  print("Warning: Rhat >=1.01")
} else {
  print("All Rhat <1.01")
}

if(min(neff_ratio(eval(M0)), na.rm=T) <= 0.2) {
  print("Warning: ESS <=0.2")
} else {
  print("All ESS >0.2")
}
```

### Posterior predictive check

```{r}
pp_check(M2, type = "bars", nsamples = 250)
```

Not much to say, but instead turn towards the next model $\mathcal{M}_3$.

## $\mathcal{M}_3$

An <font style="font-family: serif">Adjacent-category</font> model with predictors.

### Prior predictive checks

```{r M3-priprep}
p <- get_prior(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + 
                 dep + arch,
               family = acat, 
               data = d)

p$prior[1] <- "normal(0,1)"
p$prior[10] <- "normal(0,2)"
```

```{r M3-pri}
M3 <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = acat, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          sample_prior = "only",
          refresh = 0)
```

```{r}
pp_check(M3, type = "bars", nsamples = 250)
```

### Sample with data

```{r M3}
M3 <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = acat, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          refresh = 0)
```

### Diagnostics

```{r M3-diagnostics, echo=TRUE}
# Check divergences, tree depth, energy
rstan::check_hmc_diagnostics(eval(M0)$fit)

# Check rhat and ESS
if(max(rhat(eval(M0)), na.rm=T) >= 1.01) {
  print("Warning: Rhat >=1.01")
} else {
  print("All Rhat <1.01")
}

if(min(neff_ratio(eval(M0)), na.rm=T) <= 0.2) {
  print("Warning: ESS <=0.2")
} else {
  print("All ESS >0.2")
}
```

### Posterior predictive check

```{r}
pp_check(M3, type = "bars", nsamples = 250)
```

Let's move to the next model $\mathcal{M}_4$.

## $\mathcal{M}_4$

A <font style="font-family: serif">Sequential</font> model with predictors.

### Prior predictive checks

```{r M4-priprep}
p <- get_prior(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + 
                 dep + arch,
               family = sratio, 
               data = d)

p$prior[1] <- "normal(0,1)"
p$prior[10] <- "normal(0,2)"
```

```{r M4-pri}
M4 <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = sratio, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          sample_prior = "only",
          refresh = 0)
```

```{r}
pp_check(M4, type = "bars", nsamples = 250)
```

Here we see a large difference. The `sratio` family expects a decay. We'll see if the data will overcome this prior (it should given that we have $n=11110$).

### Sample with data

```{r M4}
M4 <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = sratio, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          refresh = 0)
```

### Diagnostics

```{r M4-diagnostics, echo=TRUE}
# Check divergences, tree depth, energy
rstan::check_hmc_diagnostics(eval(M0)$fit)

# Check rhat and ESS
if(max(rhat(eval(M0)), na.rm=T) >= 1.01) {
  print("Warning: Rhat >=1.01")
} else {
  print("All Rhat <1.01")
}

if(min(neff_ratio(eval(M0)), na.rm=T) <= 0.2) {
  print("Warning: ESS <=0.2")
} else {
  print("All ESS >0.2")
}
```

### Posterior predictive check

```{r}
pp_check(M4, type = "bars", nsamples = 250)
```

Finally, let's take a look at the final (for now) model $\mathcal{M}_5$.

## $\mathcal{M}_5$

A <font style="font-family: serif">Sequential</font> model with category-specific predictors.

### Prior predictive checks

```{r M5-priprep}
p <- get_prior(State ~ 1 + prio_s + crit + cs(b_val) + cs(c_val) + sh_s + 
                 kc_s + dep + cs(arch),
               family = sratio, 
               data = d)

p$prior[1] <- "normal(0,1)"
p$prior[10] <- "normal(0,2)"
```


```{r M5-pri}
M5 <- brm(State ~ 1 + prio_s + crit + cs(b_val) + cs(c_val) + sh_s + 
            kc_s + dep + cs(arch),
          family = sratio, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          sample_prior = "only",
          refresh = 0)
```

```{r}
pp_check(M5, type = "bars", nsamples = 250)
```

### Sample with data

```{r M5}
M5 <- brm(State ~ 1 + prio_s + crit + cs(b_val) + cs(c_val) + sh_s + 
            kc_s + dep + cs(arch),
          family = sratio, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          refresh = 0)
```

### Diagnostics

```{r M5-diagnostics, echo=TRUE}
# Check divergences, tree depth, energy
rstan::check_hmc_diagnostics(eval(M0)$fit)

# Check rhat and ESS
if(max(rhat(eval(M0)), na.rm=T) >= 1.01) {
  print("Warning: Rhat >=1.01")
} else {
  print("All Rhat <1.01")
}

if(min(neff_ratio(eval(M0)), na.rm=T) <= 0.2) {
  print("Warning: ESS <=0.2")
} else {
  print("All ESS >0.2")
}
```

### Posterior predictive check

```{r}
pp_check(M5, type = "bars", nsamples = 250)
```

# Model comparison

Once we've sampled our models we use LOO to compare the models' relative out of sample prediction capabilities [@vehtari17loo].

```{r loo_comp}
(l <- loo_compare(loo(M0), loo(M1), loo(M2), loo(M3), loo(M4), loo(M5)))
```

`LOO` puts $\mathcal{M}_5$ as no. 1. If we assume a $z_{\mathrm{99\%}}$-score of $2.58$ it's clear that zero is not in the interval and that $\mathcal{M}_5$ has an advantage, i.e., $\mathrm{CI}_{z_{99\%}}$[`r round(l[2,1] + c(-1,1) * l[2,2] * 2.58, 2)`]. 

We can also clearly see that adding predictors to $\mathcal{M}_1$ *clearly* has a significant effect (compared to $\mathcal{M}_0$). However, adding monotonic effects, $\mathcal{M}_2$, had very little added benefit (i.e., $\mathcal{M}_1$ vs. $\mathcal{M}_2$), while adding category-specific effects, $\mathcal{M}_5$, definitely did something positive concerning out of sample predictions (i.e., $\mathcal{M}_5$ vs. $\mathcal{M}_4$).

Also worth noting is that the <font style="font-family: serif">Cumulative</font> models are, relatively speaking, performing the worst (i.e., $\mathcal{M}_1$ and $\mathcal{M}_2$). The <font style="font-family: serif">Sequential</font> models take the two first spots, while the <font style="font-family: serif">Adjacent-category</font> model falls behind on the third spot.

If we would be interested in refining our models purely for optimal out of sample predictions we could conduct variable selection. However, in this particular case we are interested in each predictor's effect, so we'll keep them, and decide to use $\mathcal{M}_5$ as our target model, $\mathcal{M}$, for now.

```{r}
M <- M5
```

# Estimates and effects

## Parameter estimates

If we now focus on our estimates we will see that our *Intercepts*, which in this case are the borders between two categories in our outcome, are precisely estimated. However, to make sense of them we need to transform the estimates, since the values are on the $\logit$ scale.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
kable(round(fixef(M), digits=2), caption="Population-level estimates (fixed effects) from our model $\\mathcal{M}$. Crosspoints are in italics and significant population-level estimates are in bold. Note that the values are on $\\mathrm{logit}$ scale.") %>%
  row_spec(c(1:5), italic = TRUE) %>%
  row_spec(c(6:8,10:11,13:14,18:19,21:22,24:25), bold = TRUE) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

Let's take the first row, `Intercept[1]`, which was estimated to `r round(fixef(M)[1,1],2)`, i.e., 

```{r} 
inv_logit_scaled(fixef(M)[1,1])
```

What does `r round(inv_logit_scaled(fixef(M)[1,1]), 2)` mean? Well, we have an ordered categorical outcome. So `r round(inv_logit_scaled(fixef(M)[1,1]), 2) * 100`% of the probability mass, with a 95% credible interval of [`r round(inv_logit_scaled(fixef(M)[1,3]), 2)`, `r round(inv_logit_scaled(fixef(M)[1,4]), 2)`], was assigned to the first category: `Elicited, Dropped`. For `Intercept[2]`, `r round(inv_logit_scaled(fixef(M)[2,1]), 2)` [`r round(inv_logit_scaled(fixef(M)[2,3]), 2)`, `r round(inv_logit_scaled(fixef(M)[2,4]), 2)`] was assigned to: `Elicited, Dropped` **and** `Elicited, Prio, Dropped` (i.e., the first two categories combined). Honestly, this is not that interesting; we want to know if the predictors are making a difference, and how large of a difference they make.

Let's turn our attention to the other parameters. The `prio_s` parameter, `r round(fixef(M)[6,1], 2)`, would then become `r round(inv_logit_scaled(fixef(M)[6,1]), 2)` when transformed. But remember the suffix `_s`! We need to multiply with $\sigma_x$  and add $\bar{x}$ from the data, which leads to an estimate of `r round(inv_logit_scaled(fixef(M)[6,1]) * attr(d$prio_s, "scaled:scale") + attr(d$prio_s, "scaled:center"), 2)` [`r round(inv_logit_scaled(fixef(M)[6,3]) * attr(d$prio_s, "scaled:scale") + attr(d$prio_s, "scaled:center"), 2)`, `r round(inv_logit_scaled(fixef(M)[6,4]) * attr(d$prio_s, "scaled:scale") + attr(d$prio_s, "scaled:center"), 2)`]. 

The more exotic parameters are our category-specific effects `b_val`, `c_val`, and `arch`. To our knowledge this has never been used in an analysis in software engineering or computer science. We see that each parameter that is ordinal has five category-specific effects estimated (e.g., rows $11$--$15$ in the table above). These estimates indicate to what degree `b_val` affected the six outcomes in `State` (remember, our outcome consisted of six ordered categorical levels). 

If we continue taking `b_val` as an example, the $95$\% credible interval does not cross $0$ for the $1$st, $3$rd, and $4$th parameter (see the table above), and is clearly positive in the first two cases and negative in the last case. This means that `b_val` affects the intercepts (cutpoints) positively between the $1$st and $2$nd categories and between the $3$rd and $4$th categories, while the opposite is true for the negative effect on the intercept between the $4$th and $5$th categories. These effects would have been impossible to estimate without modeling it separately as we did in $\mathcal{M}$.

However, looking at point estimates is, quite frankly, not terribly useful. Let's plot the posterior probability densities for our population-level estimates on the $\logit$ scale, disregarding `Intercept`$[1,\ldots,5]$ (we are, after all, interested in the effect our predictors have on the outcome, not the outcome itself).

```{r mcmc_areas, echo=FALSE, warnings=FALSE, message=FALSE}
p <- mcmc_areas_ridges(M, regex_pars = c("b_val", "c_val", "arch"),
                  pars = c("b_prio_s","b_crit","b_sh_s","b_kc_s","b_dep"),
                  prob_outer = 0.95,
                  prob = 0.5) +
  vline_0(size=0.3) 
p + scale_y_discrete(breaks=c("b_prio_s","b_crit","b_sh_s", "b_kc_s", "b_dep", "bcs_b_val[1]", "bcs_b_val[2]", "bcs_b_val[3]", "bcs_b_val[4]", "bcs_b_val[5]", "bcs_c_val[1]", "bcs_c_val[2]", "bcs_c_val[3]", "bcs_c_val[4]", "bcs_c_val[5]", "bcs_arch[1]", "bcs_arch[2]", "bcs_arch[3]", "bcs_arch[4]", "bcs_arch[5]"),
                     labels=c("priority", "critical", "stakeholders", "key customers", "dependencies", "business value[1]", "business value[2]", "business value[3]", "business value[4]", "business value[5]", "customer value[1]", "customer value[2]", "customer value[3]", "customer value[4]", "customer value[5]", "architects[1]", "architects[2]", "architects[3]", "architects[4]", "architects[5]"))
```

Examining the above plot, from bottom to top, we can say that on the $95$%-level, the first three parameters are clearly positive or negative and do not cross $0$. The fourth parameter, `Key customers`, is not significant, with the following $95$% credible intervals [`r round(fixef(M)[9,3], 2)`, `r round(fixef(M)[9,4], 2)`]. The fifth parameter, `Dependencies`, is significant (positive) [`r round(fixef(M)[10,3], 2)`, `r round(fixef(M)[10,4], 2)`]. For `Business value`, `Customer value`, and `Architects' involvement` we see that some parameters are significant, e.g., for `Business value` the parameters $[1]$, $[3]$, and $[4]$.

To conclude what we've noticed so far: `Priority`, `Critical feature`, `Stakeholders`, and `Dependencies` are significant on $\mathrm{CI}_{95\%}$, while for `Business value`, `Customer value`, and `Architecture involvement` some categories are significant. `Key customers` is not significant.

## Conditional effects

Below we plot all conditional effects for our model. The colors represent the different ordered categories, $1,\ldots,6$, for our outcome `State`. We are particularly interested in Category $6$ (pink), i.e., the final category which indicates a released feature.

1. `Elicited, Dropped`
2. `Elicited, Prio, Dropped`
3. `Elicited, Prio, Planned, Dropped`
4. `Elicited, Prio, Planned, Implemented, Dropped`
5. `Elicited, Prio, Planned, Implemented, Tested, Dropped`
6. `Elicited, Prio, Planned, Implemented, Tested, Released`

```{r cond_effects, echo=FALSE}
conditional_effects(M, categorical = TRUE)
```

# Conclusions
One important question we would like to have an answer to is which independent variable(s) contribute(s) more for a feature to, ultimately, be *released*, i.e., is it priority, criticality, business or customer value, number of stakeholders, number of key customers, having dependencies, and/or the level of architect involvement? In the above plots the answer to our question can be found, without even having to conduct any statistical tests or examining $p$-values.

Concerning `Priority` we see that it has a very large effect for State $6$ (i.e., a feature being released). The higher the priority the more probability mass is set on State $6$. In the end it has close to $70$% of the probability mass, while the other states are not even close.

Concerning `Criticality` we see very much the same effect, albeit the uncertainty increases also. States $3$ and $6$ have, together, more than $60$% of the probability mass. However,in the `Stakeholders` plot, we see that State $6$ has very little probability mass (and much uncertainty) when number of stakeholders increases, i.e., the more stakeholders the lower the probability that a feature will be released.

`Key customers` doesn't tell us much due to, e.g., the very large uncertainty in State $6$. For `Dependencies` not much changes when it moves from $0$ to $1$.

`Business value` and `Customer value` play a small role if we look at the plots above. Additionally, it is a bit hard to untangle the plots and it might be worthwhile conducting some further analysis.

Finally, for `Architects involvement` we see that the probability for State $3$ increases when going from `none` ($1$) to `Joint design` ($5$). It is hard to conclude much other things given the uncertainty at play here.

# References

<div id="refs"></div>

# Appendix

## Computation time

```{r}
end.time <- Sys.time()
round((end.time - start.time), 1)
```

## Environment

```{r}
print(sessionInfo(), locale=FALSE)
```
