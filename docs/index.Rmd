---
title: "Feature selection analysis of 11110 requirements"
subtitle: "Replication package"
author: "R. Torkar and R. Berntsson Svensson"
date: "First revision 2020-01-17. This revision `r Sys.Date()`."
output:
  bookdown::html_document2:
    toc: true
    toc_float: true
    toc_depth: 1
    includes:
      before_body: footer.html
bibliography: refs.bib
link-citations: true
csl: elsevier-harvard.csl
header-includes:
  - \usepackage{amsmath}
  - \DeclareMathOperator{\logit}{logit}
---

<!-- PLEASE READ COMMENTS BELOW-->
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
knitr::opts_chunk$set(cache=TRUE)

# set this to num cores on your computer!
CORES = 4 

library(openxlsx) # read xlsx files
library(ggplot2) # plots
library(bayesplot) # Bayesian plots (ggplot2)
library(ggthemes) # setting themes for plots
# set global theme
theme_set(theme_tufte())
library(patchwork) # grouping plots in a nifty way
library(tidyverse)
library(kableExtra) # pretty tables in Rmd

# !!! Make sure to install cmdstanr and then cmdstan !!!
# See appendix!
options(brms.backend="cmdstanr")
options(mc.cores = CORES) # set num cores
library(brms)
```

# Introduction

## Synthetic data

In this replication package we have used empirical data under NDA. However, one can use synthetic data to evaluate our approach and reach very much the same conclusions.

Here we show how to load the synthetic dataset (from the `data/` directory in the GitHub [repository](https://github.com/torkar/feature-selection-RBS)), which can then be used in the analysis.

```{r, eval=FALSE}
d <- readRDS("data/data.rds")
```

The above line is currently not executed, since we will continue to run this analysis with the empirical data. However, by executing the above line, and *not* executing the next line, the analysis can be executed as-is with synthetic data.

## Data cleaning

First prepare the data, check for `NA`s and look at some descriptive statistics. Since we're using Excel we should be very careful when loading the data to see that nothing goes wrong (data manipulated in an arbitrary way, incompatible data types, etc.)

```{r}
# Make sure to execute the previous statement, and not this one, if you want to 
# re-run the analysis
d <- read.xlsx("data/Features.xlsx", sheet = "Features")
```

This is how the empirical data looks like. In short, `r nrow(d)` rows and `r ncol(d)` variables.^[In this text we use the words outcome and predictor for our dependent and independent variables, respectively. The word covariate can also often be used instead of predictor, even though it, in the traditional sense, means a continuous predictor.]

```{r}
str(d)
```

Each row (requirement) has a unique `ID`. The `State`, which is our outcome (dependent variable), shows how far the feature survived in the requirements process. It is of an ordered categorical type, which should be modeled with some type of <font style="font-family: serif">Cumulative</font> likelihood. There are six categories:

1. "Elicited, Dropped"                                     
2. "Elicited, Prio, Dropped"                               
3. "Elicited, Prio, Planned, Dropped"                      
4. "Elicited, Prio, Planned, Implemented, Dropped"         
5. "Elicited, Prio, Planned, Implemented, Tested, Dropped" 
6. "Elicited, Prio, Planned, Implemented, Tested, Released"

`Team.priority` is the relative priority the feature got, $\mathbb{N} = \{0,\ldots,1000\}$. `Critical.feature` is a 'Yes'/'No' answer ($\mathbb{Z}_2$). `Business.value` and `Customer.value` are also ordered categorical with four levels. The level 'No value' is an explicit choice, so it does *not* imply missingness: 

1. "No value" (explict choice)
2. "Valuable"
3. "Important"
4. "Critical"

`Stakeholders` have integers, i.e., $\mathbb{N} = \{0,\ldots,10\}$, and `Key.customers` the same, but with a different set, i.e., $\mathbb{N} = \{0,\ldots,60\}$. 

Finally, `Dependency` is $\mathbb{Z}_2$, while `Architects.involvement` is ordered categorical: 

1. "None" (explict choice)
2. "Simple"
3. "Monitoring"
4. "Active Participation"
5. "Joint Design"   

All ordered categorical predictors can be modeled as monotonic or category-specific effects, if necessary [@burkner20monotonic].

Check for `NA`s,

```{r}
table(is.na(d))
```

No `NA`s in the dataset. However, that doesn't necessarily mean that we don't have `NA`s. Some of the coding can be a representation of `NA`, e.g., 'No value'. In this particular case we know that 'No value' and 'None' in the dataset actually are values and not a representation of `NA`.

Finally, we set correct data types.

```{r}
# ordered categorical
d$State <- factor(d$State, 
                  levels = c("Elicited, Dropped", 
                             "Elicited, Prio, Dropped", 
                             "Elicited, Prio, Planned, Dropped", 
                             "Elicited, Prio, Planned, Implemented, Dropped", 
                             "Elicited, Prio, Planned, Implemented, Tested, Dropped", 
                             "Elicited, Prio, Planned, Implemented, Tested, Released"), 
                  ordered = TRUE)

# make sure we have integers for the other ordered categorical
d$Business.value <- factor(d$Business.value, 
                           levels = c("No value",
                                      "Valuable",
                                      "Important",
                                      "Critical"), 
                           ordered = TRUE)


d$Customer.value <- factor(d$Customer.value, 
                           levels = c("No value",
                                      "Valuable",
                                      "Important",
                                      "Critical"), 
                           ordered = TRUE)

d$Architects.involvement <- factor(d$Architects.involvement,
                                   levels = c("None",
                                              "Simple",
                                              "Monitoring",
                                              "Active Participation",
                                              "Joint Design"), 
                                   ordered = TRUE)
```


## Descriptive statistics

First, we see that for the outcome `State` approximately as many features are released (final stage) as dropped in the first state. We also see that it drops off after the initial state. 

```{r echo=FALSE}
ggplot(d, aes(x=as.factor(State))) + 
  geom_bar() +
  xlab("") + 
  ylab("Num. features") +
  ggtitle("Predictor: State") +
  theme_tufte() +
  theme(axis.text.x = element_text(angle=15, hjust=1))
  
```

For `Team.priority` many features have zero in priority ($5139$), and then there's a bunch of them ($1516$) that have priority set to the maximum value, i.e., $1000$.

```{r echo=FALSE}
ggplot(d, aes(x=Team.priority)) +
  geom_histogram(bins = 30) +
  xlab("") + 
  ylab("") +
  ggtitle("Predictor: Team priority") +
  theme_tufte()
```

For `Critical.feature` we have a clear emphasis on 'No'. 

```{r echo=FALSE}
ggplot(d, aes(x=Critical.feature)) +
  geom_bar() +
  xlab("") +
  ylab("") +
  ggtitle("Predictor: Critical feature") +
  theme_tufte()
```

Concerning `Business.value` and `Customer.value` they are fairly similar in their respective distribution (as one would perhaps expect). 

```{r echo=FALSE}
ggplot(d, aes(x=Business.value)) + 
  geom_bar() +
  xlab("") + 
  ylab("Num. features") +
  ggtitle("Predictor: Business value") + theme_tufte()

ggplot(d, aes(x=Customer.value)) + 
  geom_bar() +
  xlab("") + 
  ylab("Num. features") +
  ggtitle("Predictor: Customer value") + 
  theme_tufte()
```

For `Stakeholder` and `Key.customers` we see a strong emphasis on lower numbers, while for `Dependency` a clear emphasis on 'No'. 

```{r echo=FALSE}
ggplot(d, aes(x=as.factor(Stakeholders))) +
  geom_bar() +
  xlab("Num. stakeholders") +
  ylab("Frequency") +
  ggtitle("Predictor: Stakeholders") +
  theme_tufte()

ggplot(d, aes(x=as.factor(Key.customers))) +
  geom_bar() +
  xlab("Num. key customers") +
  ylab("") +
  ggtitle("Predictor: Key customers") +
  theme_tufte()
  
ggplot(d, aes(x=Dependency)) +
  geom_bar() +
  xlab("") +
  ylab("") +
  ggtitle("Predictor: Dependency") +
  theme_tufte()
```

Finally, for `Architects.involvement` one can see that in the absolute majority of the cases architects are not involved.

```{r echo=FALSE}
ggplot(d, aes(x=Architects.involvement)) + 
  geom_bar() +
  xlab("") + 
  ylab("Num. features") +
  ggtitle("Predictor: Architects' involvement") +
  theme_tufte() +
  theme(axis.text.x = element_text(angle=15, hjust=1))
```

In short, it looks sort of what one would expect, i.e., it's not hard to find answers to why the plots look the way they do. 

However, before we continue, we should standardize some of our predictors so the sampling will be easier, i.e., we simply do $(x - \bar{x})/\sigma_x$, then simply multiplying with $\sigma_x$ and adding the mean, will allow us to get back to the original scale. 

```{r scale}
# standardize and abbreviated names and change types if need be
d$prio_s <- scale(d$Team.priority)
d$sh_s <- scale(d$Stakeholders)
d$kc_s <- scale(d$Key.customers)

d$b_val <- as.integer(d$Business.value) # use int as input
d$c_val <- as.integer(d$Customer.value)
d$arch <- as.integer(d$Architects.involvement)

# Dichotomous predictors. We can set these to 1/0, but generally speaking
# one should know what one is doing and be careful doing this!
d$Critical.feature <- ifelse(d$Critical.feature == 'Yes', 1, 0)
d$Dependency <- ifelse(d$Dependency == 'Yes', 1, 0)

# only abbreviate names
d$crit <- d$Critical.feature
d$dep <- d$Dependency
```

# Model design {.tabset .tabset-fade .tabset-pills}

## $\mathcal{M}_0$

Since our outcome is of an ordered categorical nature we have many options at our hands, some of which barely existed in Bayesian modeling a few decades ago. Our outcome reminds us of a survival model, i.e., a feature needs to survive in order to reach the next stage. Taking this into account we assume that the following type of models could be an option [@burkner19ordinal]:

* <font style="font-family: serif">Cumulative</font> (single underlying continuous variable) [@samejima97]. A model with no predictors ($\mathcal{M}_0$), with predictors ($\mathcal{M}_1$), and with monotonic predictors ($\mathcal{M}_2$).
* <font style="font-family: serif">Adjacent-category</font> (mathematically convenient) [@agresti10]. A model with predictors ($\mathcal{M}_3$).
* <font style="font-family: serif">Sequential</font> model (higher response category is possible only after all lower categories are achieved) [@tutz90]. A model with predictors ($\mathcal{M}_4$) and with category-specific predictors ($\mathcal{M}_5$).

The reason we want to use monotonic or category-specific modeling of predictors is that predictor categories will not be assumed to be equidistant with respect to their effect on the outcome [@burkner20monotonic].

The <font style="font-family: serif">Cumulative</font> family is very common so let us assume a null model, $\mathcal{M}_0$, which uses this likelihood, with no predictors. Later we will compare all our models to $\mathcal{M}_0$, to ensure that adding predictors improve out of sample predictions, i.e., if we can't improve when adding predictors we might as well do other things with our time.

The outcome of the model design will be a set $M = \{\mathcal{M}_0,\ldots,\mathcal{M}_5\}$.

### Prior predictive checks
Let us see what priors such a null model needs.

```{r M0-priprep}
(p <- get_prior(State ~ 1, 
               family = cumulative, 
               data = d))
```

```{r}
# Set a wide Normal(0,2) on the the intercepts (cutpoints for our 
# scale, which is on 6 levels, i.e., 5 cutpoints)
p$prior[1] <- "normal(0,2)"
```

Sample only from the priors.

```{r M0-pri}
# simplest cumulative model we can think of
M0 <- brm(State ~ 1, 
          family = cumulative, 
          data = d, 
          # threads = threading(4), # use if 16 CPUs
          prior = p,
          control = list(adapt_delta=0.9),
          sample_prior = "only",
          refresh = 0) # avoid printing sampling progress
```

Plot the priors $y_{\mathrm{rep}}$ vs. the empirical data $y$.

```{r}
pp_check(M0, type = "bars", nsamples = 250)
```

Evidently the medians are quite evenly set along the $x$-axis, and the uncertainty is fairly uniformly distributed among the categories $1,\ldots,6$ (the bars). In short, this is what we like to see.

### Sample with data

```{r M0}
M0 <- brm(State ~ 1, 
          family = cumulative, 
          data = d, 
          # threads = threading(4),
          prior = p,
          control = list(adapt_delta=0.9),
          refresh = 0)
```

### Diagnostics

Our caterpillar plots look good for all estimated parameters (i.e., they look like fat caterpillars when the chains have mixed well).

```{r M0-caterpillar, echo=FALSE}
mcmc_trace(M0, regex_pars = "^b_") + legend_none()
```

Diagnostics such as divergences, tree depth, energy, $\widehat{R}$, and $\mathrm{ESS}$ all look good. 

```{r M0-diagnostics, echo=TRUE}
# Check divergences, tree depth, energy
rstan::check_hmc_diagnostics(eval(M0)$fit)

# Check rhat and ESS
if(max(rhat(eval(M0)), na.rm=T) >= 1.01) {
  print("Warning: Rhat >=1.01")
} else {
  print("All Rhat <1.01")
}

if(min(neff_ratio(eval(M0)), na.rm=T) <= 0.2) {
  print("Warning: ESS <=0.2")
} else {
  print("All ESS >0.2")
}
```

When using dynamic Hamiltonian Monte Carlo we have a plethora of diagnostics, which we should utilize to ensure validity and efficiency when sampling. Here follows a summary of the most common diagnostics.^[https://mc-stan.org/misc/warnings.html]

There should be no divergences since it's an indication that the posterior is biased; it arises when the posterior landscape is hard for the sample to explore (validity concern).^[https://mc-stan.org/docs/2_26/reference-manual/divergent-transitions.html] A reparameterization of the model might be necessary if these divergences remain.^[https://mc-stan.org/docs/2_26/stan-users-guide/reparameterization-section.html]

Tree depth warnings are not a validity concern but rather an efficiency concern. Reaching the maximum tree depth indicates that the sampler is terminating prematurely to avoid long execution time [@homanG14hmc].

Having low energy values (E-BFMI) is an indication of a biased posterior (validity concern) [@betancourt2016diagnosing].

The $\widehat{R}$ convergence diagnostics indicates if the independent chains converged, i.e. explored the posterior in approximately the same way (validity concern) [@vehtariGSCB21rhat].

The effective sample size ($\mathrm{ESS}$) captures how many independent draws contain the same amount of information as the dependent sample obtained by the MCMC algorithm. The higher the better. When we come closer to $0.1$ we should start worrying and in absolute numbers we should be in the hundreds for the Central Limit Theorem to hold [@betancourt2018conceptual].

Additionally, the Monte Carlo Standard Error (MCSE) has been checked for all models. The MCSE is yet another criteria that reflects effective accuracy of a Markov chain by dividing the standard deviation of the chain with the square root of its effective sample size [@betancourt2018conceptual],

\begin{equation}
\mathrm{MCSE} = \frac{\mathrm{SD}}{\sqrt{\mathrm{ESS}}}
\end{equation}

### Posterior predictive check

Let's look at a posterior predictive plot to see how well the model has estimated our $6$ levels we have in our outcome.

```{r}
pp_check(M0, type = "bars", nsamples = 250)
```

In short, very little uncertainty (i.e., the medians are quite well estimated). Let's leave $\mathcal{M}_0$ for now and add predictors to the next model, which you can read about by going back up and clicking on the tab $[2.2\ \mathcal{M}_1]$.

## $\mathcal{M}_1$

Here we'll design a <font style="font-family: serif">Cumulative</font> model with predictors.

For this and the coming models we won't report on all the steps since it will take up too much space. However, rest assured, we have conducted all the steps, just as we did for $\mathcal{M}_0$.

### Prior predictive checks

Let's set sane priors that are uniform on the outcome space.

```{r M1-priprep}
p <- get_prior(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
               family = cumulative, 
               data = d)

# Set N(0,1) on \betas and N(0,2) on the cutpoints
p$prior[1] <- "normal(0,1)"
p$prior[10] <- "normal(0,2)"
```

Sample only from the priors.

```{r M1-pri}
M1 <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = cumulative, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          sample_prior = "only",
          refresh = 0)
```

Plot the priors $y_{\mathrm{rep}}$ vs. the empirical data $y$.

```{r}
pp_check(M1, type = "bars", nsamples = 250)
```

### Sample with data

```{r M1}
M1 <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = cumulative, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          refresh = 0)
```

### Diagnostics

```{r M1-diagnostics, echo=TRUE}
# Check divergences, tree depth, energy
rstan::check_hmc_diagnostics(eval(M0)$fit)

# Check rhat and ESS
if(max(rhat(eval(M0)), na.rm=T) >= 1.01) {
  print("Warning: Rhat >=1.01")
} else {
  print("All Rhat <1.01")
}

if(min(neff_ratio(eval(M0)), na.rm=T) <= 0.2) {
  print("Warning: ESS <=0.2")
} else {
  print("All ESS >0.2")
}
```

### Posterior predictive check

```{r}
pp_check(M1, type = "bars", nsamples = 250)
```

Slight overestimation in the third category and slight underestimations in the first two categories. Shouldn't be a problem but worth noting. The next model is $\mathcal{M}_2$.

## $\mathcal{M}_2$

A <font style="font-family: serif">Cumulative</font> model with monotonic predictors.

### Prior predictive checks

```{r M2-priprep}
p <- get_prior(State ~ 1 + prio_s + crit + mo(b_val) + mo(c_val) + sh_s + 
                 kc_s + dep + mo(arch),
               family = cumulative, 
               data = d)

p$prior[1] <- "normal(0,1)"
p$prior[10] <- "normal(0,2)"
p$prior[16:18] <- "dirichlet(2)" # prior for ordered categorical
```

```{r M2-pri}
M2 <- brm(State ~ 1 + prio_s + crit + mo(b_val) + mo(c_val) + sh_s + kc_s + 
            dep + mo(arch),
          family = cumulative, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          sample_prior = "only",
          refresh = 0)
```

```{r}
pp_check(M2, type = "bars", nsamples = 250)
```

### Sample with data

```{r M2}
# simplest cumulative model we can think of
M2 <- brm(State ~ 1 + prio_s + crit + mo(b_val) + mo(c_val) + sh_s + kc_s + 
            dep + mo(arch),
          family = cumulative, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          refresh = 0)
```

### Diagnostics

```{r M2-diagnostics, echo=TRUE}
# Check divergences, tree depth, energy
rstan::check_hmc_diagnostics(eval(M0)$fit)

# Check rhat and ESS
if(max(rhat(eval(M0)), na.rm=T) >= 1.01) {
  print("Warning: Rhat >=1.01")
} else {
  print("All Rhat <1.01")
}

if(min(neff_ratio(eval(M0)), na.rm=T) <= 0.2) {
  print("Warning: ESS <=0.2")
} else {
  print("All ESS >0.2")
}
```

### Posterior predictive check

```{r}
pp_check(M2, type = "bars", nsamples = 250)
```

Not much to say, but instead turn towards the next model $\mathcal{M}_3$.

## $\mathcal{M}_3$

An <font style="font-family: serif">Adjacent-category</font> model with predictors.

### Prior predictive checks

```{r M3-priprep}
p <- get_prior(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + 
                 dep + arch,
               family = acat, 
               data = d)

p$prior[1] <- "normal(0,1)"
p$prior[10] <- "normal(0,2)"
```

```{r M3-pri}
M3 <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = acat, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          sample_prior = "only",
          refresh = 0)
```

```{r}
pp_check(M3, type = "bars", nsamples = 250)
```

### Sample with data

```{r M3}
M3 <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = acat, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          refresh = 0)
```

### Diagnostics

```{r M3-diagnostics, echo=TRUE}
# Check divergences, tree depth, energy
rstan::check_hmc_diagnostics(eval(M0)$fit)

# Check rhat and ESS
if(max(rhat(eval(M0)), na.rm=T) >= 1.01) {
  print("Warning: Rhat >=1.01")
} else {
  print("All Rhat <1.01")
}

if(min(neff_ratio(eval(M0)), na.rm=T) <= 0.2) {
  print("Warning: ESS <=0.2")
} else {
  print("All ESS >0.2")
}
```

### Posterior predictive check

```{r}
pp_check(M3, type = "bars", nsamples = 250)
```

Let's move to the next model $\mathcal{M}_4$.

## $\mathcal{M}_4$

A <font style="font-family: serif">Sequential</font> model with predictors.

### Prior predictive checks

```{r M4-priprep}
p <- get_prior(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + 
                 dep + arch,
               family = sratio, 
               data = d)

p$prior[1] <- "normal(0,1)"
p$prior[10] <- "normal(0,2)"
```

```{r M4-pri}
M4 <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = sratio, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          sample_prior = "only",
          refresh = 0)
```

```{r}
pp_check(M4, type = "bars", nsamples = 250)
```

Here we see a large difference. The `sratio` family expects a decay. We'll see if the data will overcome this prior (it should given that we have $n=11110$).

### Sample with data

```{r M4}
M4 <- brm(State ~ 1 + prio_s + crit + b_val + c_val + sh_s + kc_s + dep + arch,
          family = sratio, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          control = list(adapt_delta=0.9),
          refresh = 0)
```

### Diagnostics

```{r M4-diagnostics, echo=TRUE}
# Check divergences, tree depth, energy
rstan::check_hmc_diagnostics(eval(M0)$fit)

# Check rhat and ESS
if(max(rhat(eval(M0)), na.rm=T) >= 1.01) {
  print("Warning: Rhat >=1.01")
} else {
  print("All Rhat <1.01")
}

if(min(neff_ratio(eval(M0)), na.rm=T) <= 0.2) {
  print("Warning: ESS <=0.2")
} else {
  print("All ESS >0.2")
}
```

### Posterior predictive check

```{r}
pp_check(M4, type = "bars", nsamples = 250)
```

Finally, let's take a look at the final (for now) model $\mathcal{M}_5$.

## $\mathcal{M}_5$

A <font style="font-family: serif">Sequential</font> model with category-specific predictors.

### Prior predictive checks

```{r M5-priprep}
p <- get_prior(State ~ 1 + prio_s + crit + cs(b_val) + cs(c_val) + sh_s + 
                 kc_s + dep + cs(arch),
               family = sratio, 
               data = d)

p$prior[1] <- "normal(0,1)"
p$prior[10] <- "normal(0,2)"
```


```{r M5-pri}
M5 <- brm(State ~ 1 + prio_s + crit + cs(b_val) + cs(c_val) + sh_s + 
            kc_s + dep + cs(arch),
          family = sratio, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          sample_prior = "only",
          refresh = 0)
```

```{r}
pp_check(M5, type = "bars", nsamples = 250)
```

### Sample with data

```{r M5}
M5 <- brm(State ~ 1 + prio_s + crit + cs(b_val) + cs(c_val) + sh_s + 
            kc_s + dep + cs(arch),
          family = sratio, 
          data = d, 
          # threads = threading(4), 
          prior = p,
          refresh = 0)
```

### Diagnostics

```{r M5-diagnostics, echo=TRUE}
# Check divergences, tree depth, energy
rstan::check_hmc_diagnostics(eval(M0)$fit)

# Check rhat and ESS
if(max(rhat(eval(M0)), na.rm=T) >= 1.01) {
  print("Warning: Rhat >=1.01")
} else {
  print("All Rhat <1.01")
}

if(min(neff_ratio(eval(M0)), na.rm=T) <= 0.2) {
  print("Warning: ESS <=0.2")
} else {
  print("All ESS >0.2")
}
```

### Posterior predictive check

```{r}
pp_check(M5, type = "bars", nsamples = 250)
```

# Model comparison

Once we've sampled our set of models $M$ we use LOO to compare the models' relative out of sample prediction capabilities [@vehtari17loo].

```{r loo_comp}
(l <- loo_compare(loo(M0), loo(M1), loo(M2), loo(M3), loo(M4), loo(M5)))
```

`LOO` puts $\mathcal{M}_5$ as no. 1. If we assume a $z_{\mathrm{99\%}}$-score of $2.58$ it's clear that zero is not in the interval and that $\mathcal{M}_5$ has an advantage, i.e., $\mathrm{CI}_{z_{99\%}}$[`r round(l[2,1] + c(-1,1) * l[2,2] * 2.58, 2)`]. 

By looking at the expected log pointwise predictive density (elpd), one can see that adding predictors to $\mathcal{M}_1$ *clearly* has a significant effect (compared to $\mathcal{M}_0$). However, adding monotonic effects, $\mathcal{M}_2$, had very little added benefit (i.e., $\mathcal{M}_1$ vs. $\mathcal{M}_2$), while adding category-specific effects, $\mathcal{M}_5$, definitely did something positive concerning out of sample predictions (i.e., $\mathcal{M}_5$ vs. $\mathcal{M}_4$).

Also worth noting is that the <font style="font-family: serif">Cumulative</font> models are, relatively speaking, performing the worst (i.e., $\mathcal{M}_1$ and $\mathcal{M}_2$). The <font style="font-family: serif">Sequential</font> models take the two first spots, while the <font style="font-family: serif">Adjacent-category</font> model falls behind on the third spot.

If we would be interested in refining our models purely for optimal out of sample predictions we could conduct variable selection [@piironenPV20projpred]. However, in this particular case we are interested in each predictor's effect, so we'll keep them and decide to use $\mathcal{M}_5$ as our target model, $\mathcal{M}$, for now.

```{r}
M <- M5
```

# Estimates and effects

## Parameter estimates

To make sense of the parameter estimates we need to transform them, since the values are on the $\logit$ scale.

First, let's compare the model where we did *not* use category-specific effects ($\mathcal{M}_4$), with the model which did use them ($\mathcal{M}$). On the left side we see $\mathcal{M}_4$, while on the right side we see the 'final' model we've settled on, designated $\mathcal{M}$. 

```{r ef-no-cs, echo=FALSE}

d1 <- round(fixef(M4), 2)[-c(1:5),]
d2 <- round(fixef(M5), 2)[-c(1:5),]

d1 %>%
  kable("html", align = 'rc') %>%
    kable_styling(full_width = F, position = "float_left") %>%
    row_spec(c(1:4,5,7), bold = TRUE)
 
d2 %>%
  kable("html", align = 'rc') %>%
  kable_styling(full_width = F, position = "right") %>%
  row_spec(c(1:3,5:6,8,9,13:14,16:17,19:20), bold = TRUE)
```

There are two things to note. First, due to us modeling category-specific effects (right table) we see that we have several estimates of interest in `arch`, which is not even a significant parameter in $\mathcal{M}_4$, whcih is lacking category-specific effects. Second, for all category-specific effects we receive a much more fine-grained view of precisely which categories in each predictor are making a difference. 

To start with, let's focus on the right-hand side table consisting of estimates from $\mathcal{M}$, and turn our attention to the other parameters. The `prio_s` parameter, `r round(fixef(M)[6,1], 2)`, would then become `r round(inv_logit_scaled(fixef(M)[6,1]), 2)` when transformed with the inverse logit, i.e., 

\begin{equation}
\frac{\exp(1.22)}{\exp(1.22)+1} = 0.77
\end{equation}

But remember the suffix `_s`! We need to multiply with $\sigma_x$  and add $\bar{x}$ from the data, which leads to an estimate of `r round(inv_logit_scaled(fixef(M)[6,1]) * attr(d$prio_s, "scaled:scale") + attr(d$prio_s, "scaled:center"), 2)` $\mathrm{CI}_{95\%}$[`r round(inv_logit_scaled(fixef(M)[6,3]) * attr(d$prio_s, "scaled:scale") + attr(d$prio_s, "scaled:center"), 2)`, `r round(inv_logit_scaled(fixef(M)[6,4]) * attr(d$prio_s, "scaled:scale") + attr(d$prio_s, "scaled:center"), 2)`]. 

As already discussed, the more exotic parameters are our category-specific effects `b_val`, `c_val`, and `arch`. To our knowledge this has never been used in an analysis in software engineering or computer science. We see that each parameter that is ordinal has five category-specific effects estimated (e.g., rows $6$--$10$ in the right-hand table above). These estimates indicate to what degree `b_val` affected the six outcomes in `State` (remember, our **outcome** consisted of **six** ordered categorical levels, having **five** borders, or crosspoints, between the levels). 

If we continue taking `b_val` as an example, the $95$\% credible interval does not cross $0$ for the $1$st (positive), $3$rd (negative), and $4$th (positive) parameter (see the table above). This means that `b_val` affects the borders (cutpoints) positively between the $1$st and $2$nd categories and between the $4$th and $5$th categories, while the opposite is true for the negative effect on the intercept between the $3$rd and $4$th categories. Remember, if a border is affected positively it means that the border was shifted upwards and, hence, more probability mass was moved to lower levels! But what does this mean in practice?

If you recall, our **outcome** State had six categories,

```{r}
levels(d$State)
```

So we can claim, when analyzing these estimates, that `b_val` moves probability mass to lower levels by shifting the border up between,

```{r}
levels(d$State)[1:2]
```
and,
```{r}
levels(d$State)[4:5]
```

while it shifts the border *down* between,

```{r}
levels(d$State)[3:4]
```

Next, let's plot the posterior probability densities for our population-level estimates on the $\logit$ scale.

```{r mcmc_areas, echo=FALSE, warnings=FALSE, message=FALSE}
p <- mcmc_areas(M, regex_pars = c("b_val", "c_val", "arch"),
                  pars = c("b_prio_s","b_crit","b_sh_s","b_kc_s","b_dep"),
                  prob_outer = 0.95,
                  prob = 0.5) +
  vline_0(size=0.3)

p + scale_y_discrete(breaks=c("b_prio_s","b_crit","b_sh_s", "b_kc_s", "b_dep", "bcs_b_val[1]", "bcs_b_val[2]", "bcs_b_val[3]", "bcs_b_val[4]", "bcs_b_val[5]", "bcs_c_val[1]", "bcs_c_val[2]", "bcs_c_val[3]", "bcs_c_val[4]", "bcs_c_val[5]", "bcs_arch[1]", "bcs_arch[2]", "bcs_arch[3]", "bcs_arch[4]", "bcs_arch[5]"),
                     labels=c("priority", "critical", "stakeholders", "key customers", "dependencies", "business value[1]", "business value[2]", "business value[3]", "business value[4]", "business value[5]", "customer value[1]", "customer value[2]", "customer value[3]", "customer value[4]", "customer value[5]", "architects[1]", "architects[2]", "architects[3]", "architects[4]", "architects[5]"))
```

Examining the above plot, from bottom to top, we can say that on the $95$%-level, the first three parameters are clearly positive or negative and do not cross $0$. The fourth parameter, `Key customers`, is not significant, with the following $95$% credible intervals [`r round(fixef(M)[9,3], 2)`, `r round(fixef(M)[9,4], 2)`]. The fifth parameter, `Dependencies`, is significant (positive) [`r round(fixef(M)[10,3], 2)`, `r round(fixef(M)[10,4], 2)`]. For `Business value`, `Customer value`, and `Architects' involvement` we see that some parameters are significant.

To conclude what we've noticed so far: `Priority`, `Critical feature`, `Stakeholders`, and `Dependencies` are significant on $\mathrm{CI}_{95\%}$, while for `Business value`, `Customer value`, and `Architects' involvement` some categories are significant. `Key customers` is not significant.

## Conditional effects

Below we plot conditional effects for some of our estimates. The colors represent the different ordered categories, $1,\ldots,6$, for our outcome `State`. We are particularly interested in Category $6$, i.e., the final category which indicates a released feature.

1. `Elicited, Dropped`
2. `Elicited, Prio, Dropped`
3. `Elicited, Prio, Planned, Dropped`
4. `Elicited, Prio, Planned, Implemented, Dropped`
5. `Elicited, Prio, Planned, Implemented, Tested, Dropped`
6. `Elicited, Prio, Planned, Implemented, Tested, Released`

```{r cond_effects, echo=FALSE}
ce <- conditional_effects(M, categorical = TRUE)
```

One important question we would like to have an answer to is which independent variable(s) contribute(s) more for a feature to, ultimately, be *released*, i.e., is it priority, criticality, business or customer value, number of stakeholders, number of key customers, having dependencies, and/or the level of architect involvement? In the above plots the answer to our question can be found, without even having to conduct any statistical tests or examining $p$-values. Let's start by analyzing the predictor `Priority`.

```{r, echo = FALSE}
plot(ce, plot = FALSE)[[1]] + 
  scale_fill_colorblind() + 
  xlab("Priority (scaled)") + 
  theme_tufte()
```

Concerning `Priority` we see that it has a very large effect for State $6$ (i.e., a feature being released). The higher the priority (the more to the right) the more probability mass is set on State $6$. In the end it has close to $70$% of the probability mass, while the other states are not even close. Also worth noting is how, for State $4$ (the hump), medium priorities seem to be the recipe for reaching this stage. 

```{r, echo = FALSE}
plot(ce, plot = FALSE)[[2]] + 
  scale_fill_colorblind() + 
  scale_x_continuous(name = "Critical", breaks=c(0,1), labels = c("No", "Yes")) +
  theme_tufte()
```

For the predictor `Critical` we see some of the same effects, albeit the uncertainty increases. The clearest effect is visible for State $6$, i.e., going from No to Yes significantly increases the probability, while the opposite holds for States $1$--$3$ (logically so, since if it is critical then a requirement should be released with a higher probability, i.e., Stage $6$).

```{r, echo = FALSE, warning=FALSE}
plot(ce, plot = FALSE)[[3]] + 
  scale_fill_colorblind() + 
  xlab("Number of stakeholders") +
  theme_tufte() +
  xlim(0, 15)
```

Concerning `Number of stakeholders`, we see that virtually all states (except State $1$) has a lower probability with increasing number of stakeholders (and more uncertainty is visible). For State $1$, however, an increase in stakeholders leads to an increase in probability. One could claim this is natural since having stakeholders would lead to the requirement being considered in the first place.

```{r, echo = FALSE}
plot(ce, plot = FALSE)[[5]] + 
  scale_fill_colorblind() + 
  scale_x_continuous(name = "Dependency", breaks=c(0,1), labels = c("No", "Yes")) +
  theme_tufte()
```


For `Dependency` not much changes when it moves from No to Yes.

We saw previously that `Key customers` was not a significant parameter. If we plot it we will see why, i.e., the uncertainty is too large.

```{r kc, echo=FALSE}
plot(ce, plot = FALSE)[[4]] +
  scale_fill_colorblind() + 
  xlab("Number of key customers") + 
  theme_tufte()
```


# References

<div id="refs"></div>

# Appendix

## CmdStan

For the sampling we refrain from using [rstan](https://mc-stan.org/users/interfaces/rstan) and instead use [cmdstan](https://mc-stan.org/users/interfaces/cmdstan) through the <font style="font-family: serif">R</font> package [cmdstanr](https://mc-stan.org/cmdstanr/). Generally speaking, the community now prefer users to use `cmdstan` since it updates more frequently.

Install `cmdstanr` and `cmdstan` by,

```{r, eval=FALSE}
CORES = 4 # set to the number of available CPU cores
remotes::install_github("stan-dev/cmdstanr")
cmdstanr::install_cmdstan(cores = CORES)

# once you have brms installed you can now run brms with cmdstan instead of rstan
options(brms.backend="cmdstanr")
options(mc.cores = CORES) # set num cores
library(brms)
```

For this execution we've used,

```{r}
cmdstanr::cmdstan_version()
```

## Environment

```{r}
print(sessionInfo(), locale=FALSE)
```
